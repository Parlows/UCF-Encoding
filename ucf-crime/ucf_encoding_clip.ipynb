{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCF Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to encode the whole UCF-Crime dataset, and form a dataset of embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, we will encode each frame and text with CLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# General\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# UCA Dataset\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# UCA Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# CLIP encoding\n",
    "from transformers import CLIPModel, AutoProcessor\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "# My code\n",
    "from utils import read_uca_as_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucf_crime_path = '/media/pablo/358690d7-e500-45fb-b8f8-bc48c6be13e3/UCF-Crimes/Videos'\n",
    "\n",
    "uca_path = '/media/pablo/358690d7-e500-45fb-b8f8-bc48c6be13e3/Surveillance-Video-Understanding/UCF Annotation/json'\n",
    "\n",
    "save_path = 'clip_embs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read UCA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "uca_df = read_uca_as_df(uca_path=uca_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_duration</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sentence</th>\n",
       "      <th>video</th>\n",
       "      <th>dataset</th>\n",
       "      <th>clip_duration</th>\n",
       "      <th>class_name</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.0</td>\n",
       "      <td>[0.0, 5.3]</td>\n",
       "      <td>A woman with short hair, slightly fat, wearing...</td>\n",
       "      <td>Abuse001_x264</td>\n",
       "      <td>train</td>\n",
       "      <td>5.3</td>\n",
       "      <td>Abuse</td>\n",
       "      <td>True</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91.0</td>\n",
       "      <td>[7.0, 8.5]</td>\n",
       "      <td>A man wearing a white shirt and black pants en...</td>\n",
       "      <td>Abuse001_x264</td>\n",
       "      <td>train</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Abuse</td>\n",
       "      <td>True</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91.0</td>\n",
       "      <td>[7.2, 8.5]</td>\n",
       "      <td>A man wearing a black shirt and black pants en...</td>\n",
       "      <td>Abuse001_x264</td>\n",
       "      <td>train</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Abuse</td>\n",
       "      <td>True</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.0</td>\n",
       "      <td>[8.2, 8.9]</td>\n",
       "      <td>A man wearing a white shirt and black pants ap...</td>\n",
       "      <td>Abuse001_x264</td>\n",
       "      <td>train</td>\n",
       "      <td>0.7</td>\n",
       "      <td>Abuse</td>\n",
       "      <td>True</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.0</td>\n",
       "      <td>[8.9, 11.2]</td>\n",
       "      <td>A man in black clothes approached a short-hair...</td>\n",
       "      <td>Abuse001_x264</td>\n",
       "      <td>train</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Abuse</td>\n",
       "      <td>True</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  video_duration    timestamp  \\\n",
       "0           91.0   [0.0, 5.3]   \n",
       "1           91.0   [7.0, 8.5]   \n",
       "2           91.0   [7.2, 8.5]   \n",
       "3           91.0   [8.2, 8.9]   \n",
       "4           91.0  [8.9, 11.2]   \n",
       "\n",
       "                                            sentence          video dataset  \\\n",
       "0  A woman with short hair, slightly fat, wearing...  Abuse001_x264   train   \n",
       "1  A man wearing a white shirt and black pants en...  Abuse001_x264   train   \n",
       "2  A man wearing a black shirt and black pants en...  Abuse001_x264   train   \n",
       "3  A man wearing a white shirt and black pants ap...  Abuse001_x264   train   \n",
       "4  A man in black clothes approached a short-hair...  Abuse001_x264   train   \n",
       "\n",
       "   clip_duration class_name  anomaly  sentence_length  \n",
       "0            5.3      Abuse     True              158  \n",
       "1            1.5      Abuse     True              144  \n",
       "2            1.3      Abuse     True              144  \n",
       "3            0.7      Abuse     True              275  \n",
       "4            2.3      Abuse     True              185  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uca_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained('openai/clip-vit-large-patch14')\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_embedding(model:CLIPModel, processor:AutoProcessor, img:np.array) -> torch.FloatTensor:\n",
    "    inputs = processor(images=img, return_tensors='pt')\n",
    "\n",
    "    image_features = model.get_image_features(**inputs)\n",
    "\n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_duration</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sentence</th>\n",
       "      <th>video</th>\n",
       "      <th>dataset</th>\n",
       "      <th>clip_duration</th>\n",
       "      <th>class_name</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.0</td>\n",
       "      <td>[0.0, 5.3]</td>\n",
       "      <td>A woman with short hair, slightly fat, wearing...</td>\n",
       "      <td>Abuse001_x264</td>\n",
       "      <td>train</td>\n",
       "      <td>5.3</td>\n",
       "      <td>Abuse</td>\n",
       "      <td>True</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91.0</td>\n",
       "      <td>[7.0, 8.5]</td>\n",
       "      <td>A man wearing a white shirt and black pants en...</td>\n",
       "      <td>Abuse001_x264</td>\n",
       "      <td>train</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Abuse</td>\n",
       "      <td>True</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91.0</td>\n",
       "      <td>[7.2, 8.5]</td>\n",
       "      <td>A man wearing a black shirt and black pants en...</td>\n",
       "      <td>Abuse001_x264</td>\n",
       "      <td>train</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Abuse</td>\n",
       "      <td>True</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.0</td>\n",
       "      <td>[8.2, 8.9]</td>\n",
       "      <td>A man wearing a white shirt and black pants ap...</td>\n",
       "      <td>Abuse001_x264</td>\n",
       "      <td>train</td>\n",
       "      <td>0.7</td>\n",
       "      <td>Abuse</td>\n",
       "      <td>True</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.0</td>\n",
       "      <td>[8.9, 11.2]</td>\n",
       "      <td>A man in black clothes approached a short-hair...</td>\n",
       "      <td>Abuse001_x264</td>\n",
       "      <td>train</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Abuse</td>\n",
       "      <td>True</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  video_duration    timestamp  \\\n",
       "0           91.0   [0.0, 5.3]   \n",
       "1           91.0   [7.0, 8.5]   \n",
       "2           91.0   [7.2, 8.5]   \n",
       "3           91.0   [8.2, 8.9]   \n",
       "4           91.0  [8.9, 11.2]   \n",
       "\n",
       "                                            sentence          video dataset  \\\n",
       "0  A woman with short hair, slightly fat, wearing...  Abuse001_x264   train   \n",
       "1  A man wearing a white shirt and black pants en...  Abuse001_x264   train   \n",
       "2  A man wearing a black shirt and black pants en...  Abuse001_x264   train   \n",
       "3  A man wearing a white shirt and black pants ap...  Abuse001_x264   train   \n",
       "4  A man in black clothes approached a short-hair...  Abuse001_x264   train   \n",
       "\n",
       "   clip_duration class_name  anomaly  sentence_length  \n",
       "0            5.3      Abuse     True              158  \n",
       "1            1.5      Abuse     True              144  \n",
       "2            1.3      Abuse     True              144  \n",
       "3            0.7      Abuse     True              275  \n",
       "4            2.3      Abuse     True              185  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_duration</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sentence</th>\n",
       "      <th>video</th>\n",
       "      <th>dataset</th>\n",
       "      <th>clip_duration</th>\n",
       "      <th>class_name</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>79.34</td>\n",
       "      <td>[1.3, 18.5]</td>\n",
       "      <td>In the restaurant, two women wanted to leave a...</td>\n",
       "      <td>Assault013_x264</td>\n",
       "      <td>train</td>\n",
       "      <td>17.2</td>\n",
       "      <td>Assault</td>\n",
       "      <td>True</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>79.34</td>\n",
       "      <td>[19.0, 26.8]</td>\n",
       "      <td>Two thugs with sticks came in and beat people ...</td>\n",
       "      <td>Assault013_x264</td>\n",
       "      <td>train</td>\n",
       "      <td>7.8</td>\n",
       "      <td>Assault</td>\n",
       "      <td>True</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>79.34</td>\n",
       "      <td>[19.5, 26.0]</td>\n",
       "      <td>Several other women were also kicked out of th...</td>\n",
       "      <td>Assault013_x264</td>\n",
       "      <td>train</td>\n",
       "      <td>6.5</td>\n",
       "      <td>Assault</td>\n",
       "      <td>True</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>79.34</td>\n",
       "      <td>[41.8, 79.3]</td>\n",
       "      <td>Two or three people came in from outside, lift...</td>\n",
       "      <td>Assault013_x264</td>\n",
       "      <td>train</td>\n",
       "      <td>37.5</td>\n",
       "      <td>Assault</td>\n",
       "      <td>True</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     video_duration     timestamp  \\\n",
       "1174          79.34   [1.3, 18.5]   \n",
       "1175          79.34  [19.0, 26.8]   \n",
       "1176          79.34  [19.5, 26.0]   \n",
       "1177          79.34  [41.8, 79.3]   \n",
       "\n",
       "                                               sentence            video  \\\n",
       "1174  In the restaurant, two women wanted to leave a...  Assault013_x264   \n",
       "1175  Two thugs with sticks came in and beat people ...  Assault013_x264   \n",
       "1176  Several other women were also kicked out of th...  Assault013_x264   \n",
       "1177  Two or three people came in from outside, lift...  Assault013_x264   \n",
       "\n",
       "     dataset  clip_duration class_name  anomaly  sentence_length  \n",
       "1174   train           17.2    Assault     True              132  \n",
       "1175   train            7.8    Assault     True              103  \n",
       "1176   train            6.5    Assault     True               91  \n",
       "1177   train           37.5    Assault     True              115  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uca_df[uca_df['video'] == 'Assault013_x264']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Saving to clip_embs/Assault013_x264_39-555.npy\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Saving to clip_embs/Assault013_x264_570-804.npy\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Saving to clip_embs/Assault013_x264_585-780.npy\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Saving to clip_embs/Assault013_x264_1254-2379.npy\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n",
      "Getting emb...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGetting emb...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[43mget_img_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     clip_array[i\u001b[38;5;241m-\u001b[39mstart_frame] \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m start_frame\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m5\u001b[39m:\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mget_img_embedding\u001b[0;34m(model, processor, img)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_img_embedding\u001b[39m(model:CLIPModel, processor:AutoProcessor, img:np\u001b[38;5;241m.\u001b[39marray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m      2\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimg, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_features\n",
      "File \u001b[0;32m~/Documents/TFM/.venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:1054\u001b[0m, in \u001b[0;36mCLIPModel.get_image_features\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1049\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1050\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1051\u001b[0m )\n\u001b[1;32m   1052\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1054\u001b[0m vision_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m vision_outputs[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# pooled_output\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual_projection(pooled_output)\n",
      "File \u001b[0;32m~/Documents/TFM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/TFM/.venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:852\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    849\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values)\n\u001b[1;32m    850\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layrnorm(hidden_states)\n\u001b[0;32m--> 852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    860\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m last_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/Documents/TFM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/TFM/.venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:637\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    629\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    630\u001b[0m         encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    631\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m         output_attentions,\n\u001b[1;32m    635\u001b[0m     )\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/Documents/TFM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/TFM/.venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:384\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    382\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    383\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm2(hidden_states)\n\u001b[0;32m--> 384\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    387\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/Documents/TFM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/TFM/.venv/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:340\u001b[0m, in \u001b[0;36mCLIPMLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    339\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(hidden_states)\n\u001b[0;32m--> 340\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(hidden_states)\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/Documents/TFM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(ucf_crime_path):\n",
    "    if not dirs: # It is a category's folder\n",
    "        for video in files:\n",
    "            \n",
    "            # Get all annotations belonging to this video\n",
    "            sub_df = uca_df[uca_df['video'] == video.split('.')[0]]\n",
    "\n",
    "            # Read video\n",
    "            video_path = os.path.join(root, video)\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "            # Get timestamps\n",
    "            timestamps = sub_df['timestamp'].tolist()\n",
    "\n",
    "            sorted_timestamps = sorted(timestamps, key=lambda x: x[0])\n",
    "            \n",
    "            for timestamp in (sorted_timestamps):\n",
    "                \n",
    "                start_frame, end_frame = int(timestamp[0]*fps), int(timestamp[1]*fps)\n",
    "\n",
    "                clip_array = np.zeros((end_frame-start_frame, 768))\n",
    "\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "                for i in range(start_frame, end_frame):\n",
    "                    ret, frame = cap.read()\n",
    "                    if ret:\n",
    "                        print('Getting emb...')\n",
    "                        emb = get_img_embedding(model=model, processor=processor, img=frame)\n",
    "                        clip_array[i-start_frame] = emb.detach().numpy()\n",
    "                    if i >= start_frame+5:\n",
    "                        break\n",
    "                \n",
    "                np_file_name = video.split('.')[0] + '_' + str(start_frame) + '-' + str(end_frame) + '.npy'\n",
    "                print('Saving to ' + os.path.join(save_path, np_file_name))\n",
    "                np.save(os.path.join(save_path, np_file_name), clip_array)\n",
    "\n",
    "            cap.release()\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.77332401e-01,  1.10363793e+00,  4.30029482e-01,  9.51468527e-01,\n",
       "       -7.02912956e-02, -5.57404757e-02,  3.70428592e-01,  1.84616685e-01,\n",
       "       -4.06397104e-01, -4.25457597e-01, -3.02474052e-01,  3.71142805e-01,\n",
       "        1.02982402e-01,  4.94437546e-01,  3.68305087e-01, -3.08489144e-01,\n",
       "       -6.71047449e-01, -7.09713817e-01, -3.21384639e-01, -5.09767532e-01,\n",
       "       -3.45718592e-01, -3.01636189e-01, -1.49021745e-02, -3.47284555e-01,\n",
       "        1.87883779e-01,  5.61046481e-01,  1.07402876e-01, -2.06557512e-02,\n",
       "       -5.47952503e-02, -5.40044785e-01,  1.05459504e-01, -8.14974308e-02,\n",
       "       -2.91178256e-01, -6.75406098e-01, -1.09499300e+00,  3.00662398e-01,\n",
       "        7.41113573e-02,  3.57261077e-02, -5.07738829e-01,  3.91648591e-01,\n",
       "       -7.17744529e-02, -3.39519948e-01,  3.87987435e-01,  1.02559768e-01,\n",
       "       -4.83796537e-01, -7.34097242e-01,  3.38603199e-01,  3.17896247e-01,\n",
       "        1.75777227e-01, -1.09225130e+00, -7.26605058e-02,  1.82681888e-01,\n",
       "        2.02016830e-01,  1.96571589e-01, -4.98033985e-02, -8.81146133e-01,\n",
       "        3.58599603e-01, -2.62930334e-01,  5.23092449e-02,  2.95762897e-01,\n",
       "       -5.99812508e-01, -1.00831866e+00,  3.08990657e-01, -1.03916109e+00,\n",
       "        4.33465362e-01, -7.66637087e-01, -1.76596254e-01, -8.00379753e-01,\n",
       "        2.43621558e-01,  7.83919752e-01, -1.89035237e-01,  1.92729294e-01,\n",
       "       -4.31521982e-01, -4.50980991e-01,  4.93736565e-01,  1.41921401e-01,\n",
       "        4.40947950e-01,  2.84716457e-01,  1.77671164e-01, -6.88626468e-02,\n",
       "        2.76811481e-01, -4.60780352e-01,  7.55890906e-01, -7.75889009e-02,\n",
       "        6.54679179e-01, -3.64584953e-01,  5.28983712e-01,  1.91561490e-01,\n",
       "       -1.60754621e-02, -1.06933266e-02,  4.33517694e-01,  1.14257836e+00,\n",
       "        1.62835702e-01,  1.19762845e-01,  1.17408767e-01, -4.80756015e-02,\n",
       "        2.07605585e-01, -2.77842641e-01, -6.00150228e-02,  3.45848560e-01,\n",
       "       -1.01899409e+00,  4.24673557e-01, -3.71682286e-01,  1.15509689e-01,\n",
       "       -9.22528386e-01, -6.12906933e-01,  1.87975466e-01, -3.25836152e-01,\n",
       "        1.55526042e-01, -7.07785487e-01,  2.96965361e-01,  9.05371755e-02,\n",
       "       -1.00921109e-01,  2.52998799e-01, -4.47542250e-01, -8.63788486e-01,\n",
       "        1.15688577e-01,  4.04793441e-01, -4.25692618e-01, -8.54811668e-01,\n",
       "        6.27991557e-02, -6.58241808e-01,  4.27918792e-01,  4.37939167e-01,\n",
       "       -4.77683067e-01, -1.29694414e+00, -2.30872288e-01, -2.52765149e-01,\n",
       "       -9.53213036e-01, -1.40681839e+00,  1.43624234e+00, -1.90949842e-01,\n",
       "       -2.86457390e-02, -6.62043214e-01,  1.32391989e-01,  5.54599643e-01,\n",
       "       -1.53949469e-01, -1.31999373e-01,  2.12346137e-01, -2.98720837e-01,\n",
       "        6.62288904e-01,  4.13169324e-01, -4.04957443e-01, -1.10152495e+00,\n",
       "        9.47188258e-01, -5.03252625e-01, -2.80503750e-01, -1.41055673e-01,\n",
       "        2.35067114e-01, -1.32664651e-01,  7.54734278e-01, -6.42452314e-02,\n",
       "        2.97456473e-01,  1.60417914e-01, -3.38899642e-02, -5.98802745e-01,\n",
       "       -3.42150062e-01, -4.01551276e-02, -6.38203919e-01, -6.14152104e-02,\n",
       "       -2.27938116e-01,  1.88909307e-01,  5.81386387e-02, -2.21016556e-02,\n",
       "       -7.40625620e-01, -7.48758197e-01, -9.85734224e-01,  3.39600086e-01,\n",
       "       -6.57369614e-01, -1.40104100e-01, -2.82422096e-01,  2.74489164e-01,\n",
       "       -4.36126828e-01, -5.12956858e-01, -4.30059403e-01, -8.34230781e-01,\n",
       "       -9.90651727e-01, -1.22381341e+00,  1.00265014e+00,  1.67239726e-01,\n",
       "        4.10776764e-01, -4.87465203e-01, -3.02617382e-02,  4.80472147e-02,\n",
       "       -4.20237541e-01,  2.05295205e-01, -1.15557647e+00, -3.36959273e-01,\n",
       "       -8.85573477e-02, -4.75621611e-01,  1.39225855e-01,  1.78890377e-02,\n",
       "       -7.02145472e-02,  2.66701519e-01, -8.93597960e-01, -1.05580378e+00,\n",
       "       -5.41169405e-01,  1.49845392e-01, -6.11171603e-01, -3.56415600e-01,\n",
       "        3.04927886e-01,  1.84065056e+00, -1.13077894e-01, -1.99946284e-01,\n",
       "       -3.39177132e-01,  2.22327530e-01, -2.07578227e-01,  1.58047378e-02,\n",
       "        7.97230303e-02,  4.66112077e-01,  4.08556104e-01, -4.32174802e-01,\n",
       "        3.13821197e-01,  9.83745754e-02,  5.41290283e-01, -2.54106075e-01,\n",
       "        6.63826346e-01, -2.68362641e-01,  3.08487117e-01, -9.38819289e-01,\n",
       "       -4.56648588e-01,  3.67004722e-02, -8.96330029e-02,  5.30797064e-01,\n",
       "       -4.43917036e-01, -5.53962529e-01,  6.53602362e-01, -9.18306187e-02,\n",
       "       -7.52276182e-03, -6.85417235e-01, -1.91941351e-01, -6.83021009e-01,\n",
       "       -5.60729682e-01,  3.89808834e-01, -2.65608877e-01,  6.15172923e-01,\n",
       "        1.50630802e-01,  1.93692327e-01,  2.71320462e-01,  1.96909010e-02,\n",
       "       -4.36600387e-01,  7.92082012e-01, -2.14472696e-01, -2.77604282e-01,\n",
       "        6.93636388e-02,  2.94369072e-01,  3.67033958e-01,  2.63203144e-01,\n",
       "       -3.00667346e-01,  6.88580051e-02,  1.06590860e-01, -4.48211223e-01,\n",
       "       -4.20328856e-01,  5.65392673e-01, -2.03709215e-01, -9.07529593e-02,\n",
       "       -1.95947737e-01, -6.21881187e-01, -5.78784406e-01, -2.13262737e-01,\n",
       "       -1.12550402e+00, -4.47821617e-03,  2.53754288e-01,  2.64438957e-01,\n",
       "       -3.10872972e-01, -7.96873719e-02,  4.86901402e-02,  5.27796805e-01,\n",
       "       -2.00020313e-01,  1.10034192e+00,  4.75599587e-01, -6.04624569e-01,\n",
       "        1.79160237e-02,  2.37507343e-01, -2.74255574e-02,  7.82704413e-01,\n",
       "        4.04933214e-01, -5.56465745e-01,  5.03879964e-01,  2.60400087e-01,\n",
       "       -6.71635628e-01, -6.98435530e-02,  2.99786925e-01, -5.15033245e-01,\n",
       "       -3.55471462e-01,  2.67410755e-01, -2.00002670e-01,  3.35101783e-01,\n",
       "       -5.78680575e-01,  9.48728263e-01, -3.40165555e-01, -9.78544503e-02,\n",
       "        7.71532118e-01, -4.06968474e-01, -3.84757757e-01, -1.50140360e-01,\n",
       "       -2.24286243e-01,  9.77277160e-02, -3.81391227e-01, -1.84738308e-01,\n",
       "        8.80135894e-02, -1.56444132e-01, -9.22470540e-02,  3.33356917e-01,\n",
       "       -1.77191257e-01, -5.74611187e-01, -1.74631789e-01, -3.59676361e-01,\n",
       "        1.63699210e-01, -4.69330013e-01, -1.35628986e+00,  4.50335860e-01,\n",
       "        1.49954066e-01, -3.11165869e-01,  2.69812822e-01, -1.43769667e-01,\n",
       "       -8.99199128e-01, -2.40612403e-02,  3.70359063e-01, -4.82697308e-01,\n",
       "       -1.63161159e-01, -5.20682216e-01, -3.14999789e-01, -6.51590228e-01,\n",
       "       -2.78265417e-01,  3.12255502e-01, -2.75488019e-01,  5.19633472e-01,\n",
       "       -7.99471676e-01, -1.10887527e+00,  2.41389573e-01, -1.24948405e-01,\n",
       "       -2.64935106e-01, -1.87781170e-01,  6.47265315e-01, -3.18025649e-01,\n",
       "       -1.19933188e-01, -3.59371603e-02, -2.73871332e-01, -6.03729710e-02,\n",
       "        1.22210197e-01,  4.21483576e-01, -1.95579618e-01, -3.69586498e-01,\n",
       "       -4.14965063e-01,  1.48112297e-01, -4.02081013e-02,  2.11728621e+00,\n",
       "        5.93012869e-02, -6.95038587e-03, -9.04904246e-01,  8.87882113e-02,\n",
       "       -3.96163240e-02,  1.70223743e-01, -7.96250582e-01,  4.66759533e-01,\n",
       "        4.94524091e-01,  9.84654903e-01,  5.26928067e-01,  7.88793921e-01,\n",
       "       -1.56560987e-01,  3.68542016e-01,  4.68740821e-01, -1.55355513e+00,\n",
       "       -5.00116587e-01,  1.91669986e-02,  4.59019184e-01, -1.07811227e-01,\n",
       "       -2.82823086e-01,  7.26500809e-01,  3.49973857e-01,  2.58651048e-01,\n",
       "       -3.18266511e-01,  4.55244184e-02, -6.33625031e-01,  5.06717205e-01,\n",
       "        6.95642471e-01, -4.40974057e-01, -7.65437782e-02,  3.03031951e-01,\n",
       "        9.63438511e-01,  1.43575847e-01, -6.39459491e-01, -8.38878453e-01,\n",
       "       -2.85568237e-01, -7.50709921e-02,  1.47952035e-01,  3.46585214e-01,\n",
       "        3.40901732e-01,  7.88528398e-02,  1.48674345e+00,  2.26134658e-02,\n",
       "       -3.12366486e-02, -6.87648356e-03, -5.25109708e-01,  5.19792557e-01,\n",
       "       -3.65084589e-01,  9.88357067e-01,  2.46066824e-01,  1.42234080e-02,\n",
       "        2.85128951e-02,  5.22984743e-01,  2.80226856e-01, -3.49327087e-01,\n",
       "        1.28340364e-01, -1.58560723e-01,  1.08572304e-01,  7.16338217e-01,\n",
       "        2.27915376e-01,  2.89990306e-01, -1.14053041e-01, -2.54812866e-01,\n",
       "        3.29551518e-01, -5.61467552e+00,  2.50068784e-01,  5.41598856e-01,\n",
       "       -4.88938749e-01,  5.44931471e-01, -6.27144933e-01, -1.15531556e-01,\n",
       "        6.47478223e-01, -2.51777947e-01,  1.74228579e-01, -3.19731206e-01,\n",
       "        1.49034768e-01, -2.24060506e-01,  7.27011338e-02, -1.16900817e-01,\n",
       "       -6.56422496e-01,  1.69543028e-02,  4.44805324e-01, -6.29220903e-01,\n",
       "       -3.44679564e-01,  3.83354366e-01, -2.10719049e-01,  6.79650724e-01,\n",
       "       -2.48085171e-01, -2.98568010e-02, -4.69293296e-02,  5.48063397e-01,\n",
       "        7.93070436e-01, -2.56330937e-01,  5.61010718e-01,  9.20238912e-01,\n",
       "        9.61459279e-01, -1.95386529e-01, -6.33317590e-01,  5.19066691e-01,\n",
       "       -2.05883354e-01, -6.43007755e-01,  7.14860320e-01,  9.61944520e-01,\n",
       "       -1.31473875e+00, -2.12926000e-01,  1.38290477e+00, -3.16463917e-01,\n",
       "        4.95506823e-01,  3.84980232e-01,  8.65808129e-02,  3.72076392e-01,\n",
       "       -8.93551052e-01,  7.23937869e-01,  5.12081921e-01,  1.73842299e+00,\n",
       "       -1.47862768e+00, -9.54230577e-02, -7.24673718e-02,  4.22940063e+00,\n",
       "        3.92679423e-02,  6.50753200e-01, -9.91626307e-02,  3.58176410e-01,\n",
       "        3.61771882e-02,  4.91400361e-01,  2.33345523e-01,  4.05218363e-01,\n",
       "       -9.81126726e-03,  3.47673625e-01, -4.65269446e-01,  1.08466044e-01,\n",
       "       -4.71518159e-01,  2.05714405e-01, -2.94791073e-01, -4.38770086e-01,\n",
       "        5.01728132e-02,  4.23014849e-01,  2.14120120e-01,  3.35446596e-02,\n",
       "       -1.16342813e-01,  8.22167397e-02,  6.21855497e-01, -5.58737993e-01,\n",
       "        3.82828593e-01,  3.29999059e-01, -2.39054963e-01,  2.57721156e-01,\n",
       "        3.62108350e-01,  1.09936297e-02,  3.25058699e-01, -7.00605154e-01,\n",
       "        5.07646278e-02,  1.81686655e-01, -1.53151304e-02, -1.86899975e-02,\n",
       "        2.17641219e-01, -3.00581425e-01,  8.84413779e-01,  1.39111683e-01,\n",
       "       -1.81138563e+00,  8.89787003e-02,  1.01682329e+00, -4.13510323e-01,\n",
       "        1.00458324e+00,  6.82880521e-01,  3.88486415e-01,  9.47845578e-02,\n",
       "        1.52373314e-03, -2.17095256e-01,  5.98588467e-01, -2.66315937e-01,\n",
       "        5.11786103e-01, -7.54167378e-01,  3.73153210e-01, -1.78776190e-01,\n",
       "       -3.95124525e-01,  8.33875388e-02,  8.66233170e-01, -6.91490412e-01,\n",
       "       -7.03343987e-01, -5.64529300e-02, -7.69254267e-01, -3.90219957e-01,\n",
       "       -3.22163701e-01, -1.29789114e-02, -5.65704048e-01, -9.09875691e-01,\n",
       "       -2.24690184e-01, -1.40345231e-01,  3.59925508e-01,  7.21024752e-01,\n",
       "        1.65779486e-01, -2.07055882e-01, -4.79600877e-01, -9.14635062e-01,\n",
       "       -2.90614069e-01, -6.93663359e-01, -1.61693335e-01, -1.69880271e-01,\n",
       "       -2.10924447e-01, -1.34220824e-01, -7.19340742e-01, -1.76202506e-01,\n",
       "       -3.91734481e-01,  1.39801919e-01,  6.71845198e-01,  1.56864941e-01,\n",
       "       -1.84591830e+00,  2.93691158e-01,  5.48227131e-02,  6.85585141e-02,\n",
       "        3.92578691e-01, -1.68806076e-01, -7.06912130e-02,  7.36181974e-01,\n",
       "        7.45273754e-02,  5.05215049e-01,  2.97663063e-01, -9.94139910e-03,\n",
       "       -7.61394739e-01, -3.51044387e-01, -3.89673084e-01,  8.54015708e-01,\n",
       "       -1.04751992e+00,  2.02819973e-01,  4.33586359e-01,  4.94265795e-01,\n",
       "       -1.14253879e-01,  4.26793039e-01, -1.50266886e-01, -5.30550539e-01,\n",
       "        3.58811975e-01,  5.58268577e-02,  3.45366925e-01,  3.73674273e-01,\n",
       "        9.88195419e-01, -3.90868306e-01, -2.82980859e-01, -1.56701520e-01,\n",
       "       -1.15901005e+00, -4.56013381e-02, -7.59265184e-01,  2.10739672e-02,\n",
       "        1.22177267e+00,  5.49021602e-01, -4.30042475e-01,  2.39484400e-01,\n",
       "        3.43636692e-01, -1.07406080e+00, -1.18136168e+00, -6.89005613e-01,\n",
       "       -3.38517308e-01,  5.13218820e-01, -2.97280759e-01,  5.21129370e-01,\n",
       "        1.07666582e-01, -1.09950259e-01,  2.37735987e-01, -3.41603249e-01,\n",
       "       -5.84602058e-01,  1.92861885e-01,  2.50930369e-01,  2.90713668e-01,\n",
       "        4.97630209e-01, -3.13237607e-01,  4.62965786e-01, -6.40517652e-01,\n",
       "        8.27502385e-02, -1.44757843e+00,  8.04719388e-01,  3.85214686e-01,\n",
       "        4.26828563e-01,  4.20882016e-01,  1.20162487e-01, -1.62804484e-01,\n",
       "        7.97131062e-01, -1.04253924e+00,  2.57144868e-01,  5.65345585e-01,\n",
       "       -2.75074959e-01,  3.47854495e-01, -2.85095334e-01,  3.55250895e-01,\n",
       "        3.91252220e-01,  5.30984998e-02,  4.00506794e-01, -7.75298476e-03,\n",
       "       -9.38208699e-02,  2.31177598e-01,  1.13077223e-01, -6.84959948e-01,\n",
       "       -8.10483754e-01,  2.63727605e-01,  1.70845836e-01,  2.23299176e-01,\n",
       "        6.03283024e+00,  1.02258873e+00,  4.81735170e-02, -1.90369725e-01,\n",
       "        9.15145501e-02, -3.18513989e-01,  1.75344706e-01,  2.48843700e-01,\n",
       "       -2.41852075e-01, -2.61378348e-01, -7.47999907e-01, -2.15859264e-02,\n",
       "       -4.47512776e-01,  2.05822796e-01, -3.41865897e-01,  9.04458225e-01,\n",
       "        1.47753000e-01,  3.16024333e-01,  2.54296094e-01,  2.09538698e-01,\n",
       "       -4.05984074e-02,  3.95804495e-01, -3.90759587e-01, -5.35556316e-01,\n",
       "       -1.29369512e-01,  5.38358390e-01, -1.16614866e+00, -3.80407572e-01,\n",
       "       -7.44760811e-01,  5.48451364e-01,  5.27723074e-01,  2.42447495e-01,\n",
       "       -3.79880011e-01,  1.74758479e-01, -4.05366778e-01, -1.34451926e-01,\n",
       "        3.10434431e-01,  4.20256287e-01,  3.79968047e-01,  6.73747063e-01,\n",
       "       -6.79985285e-01,  1.12204158e+00,  2.86829740e-01,  3.88497859e-01,\n",
       "       -4.64953035e-01,  2.09504932e-01,  4.78332400e-01,  7.69969702e-01,\n",
       "        8.76631677e-01,  9.46658850e-03,  5.08070052e-01, -6.15262389e-01,\n",
       "        4.56867665e-02,  3.98896247e-01,  5.09453416e-01, -4.14354354e-01,\n",
       "       -6.12714812e-02, -2.34769821e-01, -2.66878843e-01, -8.88568044e-01,\n",
       "        1.58140987e-01,  1.93628281e-01,  3.79910320e-02,  5.73375285e-01,\n",
       "       -1.10610373e-01,  4.75174040e-01, -6.60117805e-01,  5.56885153e-02,\n",
       "       -3.23377162e-01,  8.89389157e-01,  5.06552458e-01, -3.79639715e-01,\n",
       "       -1.60915315e-01, -3.64012599e-01, -4.83245313e-01,  1.09670913e+00,\n",
       "        4.58554357e-01,  2.44946823e-01, -1.20437786e-01, -3.74483347e-01,\n",
       "       -4.18753803e-01,  6.87275231e-01,  1.40472963e-01, -8.33382249e-01,\n",
       "        2.34164745e-01,  2.94164270e-01, -3.82911831e-01, -1.43410534e-01,\n",
       "        2.41099805e-01, -1.57018900e-02,  1.64099842e-01,  1.47780344e-01,\n",
       "        7.89782524e-01, -7.36020058e-02, -1.00195780e-01,  2.42009759e-04,\n",
       "        9.89176482e-02,  3.56011331e-01, -2.67989635e-01,  3.24269205e-01,\n",
       "        8.07237551e-02, -2.04967201e-01,  7.26401091e-01, -2.28240743e-01,\n",
       "        3.87058318e-01, -1.08524218e-01,  7.02097774e-01,  1.72856376e-01,\n",
       "        8.87789056e-02,  5.95619202e-01, -8.21161747e-01, -1.59061879e-01,\n",
       "        2.02449113e-02,  1.12173522e+00,  7.82419503e-01, -6.12013519e-01,\n",
       "        2.98964143e-01, -6.81953788e-01,  2.34996587e-01, -2.54160464e-01,\n",
       "        6.37397945e-01,  2.93799043e-01,  1.11203998e-01,  4.72526312e-01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('clip_embs/Assault013_x264_570-804.npy')[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
